#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ”¤ Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ NOTAM Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ°Ğ²Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ NOTAM Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python3 notam_decoder.py "Ñ‚ĞµĞºÑÑ‚ NOTAM"
    python3 notam_decoder.py --file notam_data.json
    python3 notam_decoder.py --file notam_data.json --icao UAAA
"""

import re
import json
import argparse
from datetime import datetime
from typing import Dict, List, Optional

# ========================================
# Ğ¡ĞŸĞ ĞĞ’ĞĞ§ĞĞ˜ĞšĞ˜ Q-ĞšĞĞ”ĞĞ’
# ========================================

Q_CODES = {
    # ĞÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ñ‹
    'QFALC': 'Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚',
    'QFALT': 'Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½',
    'QFAXX': 'Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    
    # Ğ’ĞŸĞŸ (Runway)
    'QMRLC': 'Ğ’ĞŸĞŸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°',
    'QMRLT': 'Ğ’ĞŸĞŸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°',
    'QMRXX': 'Ğ’ĞŸĞŸ: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QMRAS': 'Ğ’ĞŸĞŸ: Ğ´Ğ»Ğ¸Ğ½Ğ° ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ°',
    'QMRCC': 'Ğ’ĞŸĞŸ: ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¾',
    
    # Ğ ÑƒĞ»Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¸ (Taxiway)
    'QMXLC': 'Ñ€ÑƒĞ»Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°',
    'QMXLT': 'Ñ€ÑƒĞ»Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°',
    'QMXXX': 'Ñ€ÑƒĞ»Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ°: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    
    # ĞŸĞµÑ€Ñ€Ğ¾Ğ½Ñ‹ Ğ¸ ÑÑ‚Ğ¾ÑĞ½ĞºĞ¸ (Apron/Parking)
    'QMALC': 'Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚',
    'QMALT': 'Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚',
    'QMAXX': 'Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    
    # ĞĞ³Ğ½Ğ¸ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ° (Lights)
    'QMGXX': 'Ğ¾Ğ³Ğ½Ğ¸ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ°: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QMGLU': 'Ğ¾Ğ³Ğ½Ğ¸ Ğ’ĞŸĞŸ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚',
    'QMGLT': 'Ğ¾Ğ³Ğ½Ğ¸ Ñ€ÑƒĞ»Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞµĞº Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚',
    'QMGLA': 'Ğ¾Ğ³Ğ½Ğ¸ Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½Ğ° Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚',
    
    # ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ° (Navaids)
    'QNIAS': 'ILS Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'QNIAT': 'ILS Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'QNIXX': 'ILS: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QNVXX': 'VOR: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QNVAU': 'VOR Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'QNDXX': 'DME: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QNDAU': 'DME Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'QNNXX': 'NDB: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QNNAU': 'NDB Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    
    # Ğ’Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ (Airspace)
    'QRRCA': 'Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾',
    'QRRCT': 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°',
    'QRPCA': 'Ğ·Ğ°Ğ¿Ñ€ĞµÑ‚Ğ½Ğ°Ñ Ğ·Ğ¾Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°',
    'QRDCA': 'Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ·Ğ¾Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°',
    'QRTCA': 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ¾Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°',
    'QRAXX': 'Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    
    # ĞŸÑ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ñ (Obstacles)
    'QOBXX': 'Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ğµ: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QOBCE': 'Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾',
    'QOBCL': 'Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¾',
    
    # Ğ¡Ğ²ÑĞ·ÑŒ (Communications)
    'QFAXX': 'ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ° ÑĞ²ÑĞ·Ğ¸: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QFPXX': 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ»Ñ‘Ñ‚Ğ¾Ğ²: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    
    # ĞœĞµÑ‚ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ
    'QWFXX': 'Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QWEAU': 'Ğ¼ĞµÑ‚ĞµĞ¾ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    
    # Ğ£ÑĞ»ÑƒĞ³Ğ¸ (Services)
    'QSAXX': 'Ğ°ÑÑ€Ğ¾Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QSXXX': 'ÑĞ»ÑƒĞ¶Ğ±Ñ‹: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QSFAU': 'Ñ‚Ğ¾Ğ¿Ğ»Ğ¸Ğ²Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾',
    'QSUAS': 'Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾-ÑĞ¿Ğ°ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ»ÑƒĞ¶Ğ±Ğ°: Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
    'QSGAS': 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾',
    
    # ĞŸÑ€Ğ¾Ñ‡ĞµĞµ
    'QXXXX': 'Ğ¿Ñ€Ğ¾Ñ‡ĞµĞµ',
}

# ========================================
# Ğ¡ĞŸĞ ĞĞ’ĞĞ§ĞĞ˜Ğš ĞĞ’Ğ˜ĞĞ¦Ğ˜ĞĞĞĞ«Ğ¥ ĞĞ‘Ğ‘Ğ Ğ•Ğ’Ğ˜ĞĞ¢Ğ£Ğ 
# ========================================

ABBREVIATIONS = {
    # ĞÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹
    'RWY': 'Ğ’ĞŸĞŸ',
    'TWY': 'Ñ€ÑƒĞ»Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ°',
    'APRON': 'Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½',
    'TERMINAL': 'Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»',
    'PARKING': 'ÑÑ‚Ğ¾ÑĞ½ĞºĞ°',
    
    # Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ
    'CLSD': 'Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚',
    'CLOSED': 'Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚',
    'OPEN': 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚',
    'AVBL': 'Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½',
    'AVAILABLE': 'Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½',
    'U/S': 'Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'UNSERVICEABLE': 'Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'OPS': 'ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ',
    'OPR': 'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚',
    'OPERATIONAL': 'Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ',
    
    # ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹
    'MAINT': 'Ñ‚ĞµÑ…Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ',
    'MAINTENANCE': 'Ñ‚ĞµÑ…Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ',
    'WIP': 'ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹',
    'WORK IN PROGRESS': 'ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹',
    'CONST': 'ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾',
    'CONSTRUCTION': 'ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾',
    'REPAIR': 'Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚',
    'INSP': 'Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ñ',
    'INSPECTION': 'Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ñ',
    
    # ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ
    'ILS': 'ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ°Ğ´ĞºĞ¸',
    'VOR': 'Ğ²ÑĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ¼Ğ°ÑĞº',
    'DME': 'Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ',
    'NDB': 'Ğ½ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ¼Ğ°ÑĞº',
    'PAPI': 'Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ³Ğ»Ğ¸ÑÑĞ°Ğ´Ñ‹',
    'VASIS': 'Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ°Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¿Ğ¾ÑĞ°Ğ´ĞºÑƒ',
    
    # ĞĞ³Ğ½Ğ¸
    'LGT': 'Ğ¾Ğ³Ğ½Ğ¸',
    'LIGHTS': 'Ğ¾Ğ³Ğ½Ğ¸',
    'ALS': 'Ğ¾Ğ³Ğ½Ğ¸ Ğ·Ğ°Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¿Ğ¾ÑĞ°Ğ´ĞºÑƒ',
    'EDGE': 'ĞºÑ€Ğ¾Ğ¼Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ³Ğ½Ğ¸',
    'CL': 'Ğ¾ÑĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ğ½Ğ¸',
    'CENTERLINE': 'Ğ¾ÑĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ğ½Ğ¸',
    
    # Ğ’Ñ‹ÑĞ¾Ñ‚Ñ‹
    'SFC': 'Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒ',
    'GND': 'Ğ·ĞµĞ¼Ğ»Ñ',
    'AGL': 'Ğ½Ğ°Ğ´ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ·ĞµĞ¼Ğ»Ğ¸',
    'AMSL': 'Ğ½Ğ°Ğ´ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ¼Ğ¾Ñ€Ñ',
    'FT': 'Ñ„ÑƒÑ‚Ğ¾Ğ²',
    'FL': 'ÑÑˆĞµĞ»Ğ¾Ğ½',
    
    # Ğ’Ñ€ĞµĞ¼Ñ
    'DAILY': 'ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾',
    'MON': 'Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº',
    'TUE': 'Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¸Ğº',
    'WED': 'ÑÑ€ĞµĞ´Ğ°',
    'THU': 'Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ³',
    'FRI': 'Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ğ°',
    'SAT': 'ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ğ°',
    'SUN': 'Ğ²Ğ¾ÑĞºÑ€ĞµÑĞµĞ½ÑŒĞµ',
    'UTC': 'Ğ²ÑĞµĞ¼Ğ¸Ñ€Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ',
    'PERM': 'Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾',
    'TEMPO': 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾',
    
    # ĞŸÑ€Ğ¾Ñ‡ĞµĞµ
    'INFO': 'Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ',
    'ADZ': 'Ğ·Ğ¾Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ°',
    'CTR': 'Ğ´Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€ÑĞºĞ°Ñ Ğ·Ğ¾Ğ½Ğ°',
    'FIR': 'Ñ€Ğ°Ğ¹Ğ¾Ğ½ Ğ¿Ğ¾Ğ»Ñ‘Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸',
    'TMA': 'Ğ´Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€ÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ¹Ğ¾Ğ½',
    'FREQ': 'Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°',
    'ATIS': 'Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ',
}

# ========================================
# Ğ”Ğ•ĞšĞĞ”Ğ•Ğ  NOTAM
# ========================================

class NOTAMDecoder:
    """Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ NOTAM Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚"""
    
    def __init__(self):
        pass
    
    def decode(self, notam_data: Dict) -> Dict:
        """
        Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ NOTAM
        
        Args:
            notam_data: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ NOTAM
            
        Returns:
            Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        """
        decoded = {
            'id': notam_data.get('id'),
            'type': notam_data.get('type'),
            'type_text': self._decode_type(notam_data.get('type')),
            'severity': None,
            'severity_emoji': None,
            'q_code': notam_data.get('q_code'),
            'q_decoded': None,
            'location': notam_data.get('location'),
            'location_name': None,
            'valid_from': notam_data.get('valid_from'),
            'valid_to': notam_data.get('valid_to'),
            'is_permanent': notam_data.get('is_permanent', False),
            'schedule': notam_data.get('schedule'),
            'description_raw': notam_data.get('description_raw'),
            'description_decoded': None,
            'lower_limit': notam_data.get('lower_limit'),
            'upper_limit': notam_data.get('upper_limit'),
            'human_readable': '',
            'raw': notam_data.get('raw', '')
        }
        
        # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Q-ĞºĞ¾Ğ´
        if decoded['q_code']:
            decoded['q_decoded'] = Q_CODES.get(decoded['q_code'], decoded['q_code'])
            decoded['severity'] = self._classify_severity(decoded['q_code'])
            decoded['severity_emoji'] = self._get_severity_emoji(decoded['severity'])
        
        # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
        if decoded['description_raw']:
            decoded['description_decoded'] = self._decode_description(decoded['description_raw'])
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸
        decoded['location_name'] = self._get_location_name(decoded['location'])
        
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
        decoded['human_readable'] = self._generate_human_text(decoded)
        
        return decoded
    
    def _decode_type(self, notam_type: str) -> str:
        """Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ° NOTAM"""
        types = {
            'N': 'Ğ½Ğ¾Ğ²Ñ‹Ğ¹',
            'R': 'Ğ·Ğ°Ğ¼ĞµĞ½Ğ°',
            'C': 'Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ°',
        }
        return types.get(notam_type, notam_type)
    
    def _classify_severity(self, q_code: str) -> str:
        """
        ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ NOTAM
        
        Returns:
            'critical', 'warning', Ğ¸Ğ»Ğ¸ 'info'
        """
        # ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹ (Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ°, Ğ’ĞŸĞŸ, Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼)
        critical_patterns = [
            'QFALC',  # Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚
            'QMRLC',  # Ğ’ĞŸĞŸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°
            'QMXLC',  # Ğ Ğ” Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°
            'QNIAS',  # ILS Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
        ]
        
        # ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ñ‹ (Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ)
        warning_patterns = [
            'QMRLT',  # Ğ’ĞŸĞŸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°
            'QMALC',  # Ğ¿ĞµÑ€Ñ€Ğ¾Ğ½ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚
            'QMGLU',  # Ğ¾Ğ³Ğ½Ğ¸ Ğ’ĞŸĞŸ
            'QRRCA',  # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°
            'QOBXX',  # Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ñ
        ]
        
        if q_code in critical_patterns:
            return 'critical'
        
        for pattern in warning_patterns:
            if q_code.startswith(pattern[:4]):
                return 'warning'
        
        return 'info'
    
    def _get_severity_emoji(self, severity: str) -> str:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ emoji Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸"""
        emojis = {
            'critical': 'ğŸ”´',
            'warning': 'ğŸŸ¡',
            'info': 'ğŸ”µ',
        }
        return emojis.get(severity, 'âšª')
    
    def _decode_description(self, description: str) -> str:
        """
        Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ NOTAM Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ¹ Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€
        
        Args:
            description: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
            
        Returns:
            Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
        """
        decoded = description
        
        # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹
        for abbr, translation in ABBREVIATIONS.items():
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹
            pattern = r'\b' + re.escape(abbr) + r'\b'
            decoded = re.sub(pattern, translation, decoded, flags=re.IGNORECASE)
        
        return decoded
    
    def _get_location_name(self, location: str) -> Optional[str]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ°"""
        from collect_notam import AIRPORTS
        
        airport_info = AIRPORTS.get(location)
        if airport_info:
            return airport_info['name']
        
        return location
    
    def _generate_human_text(self, decoded: Dict) -> str:
        """
        Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°
        
        Args:
            decoded: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            
        Returns:
            Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
        """
        lines = []
        
        # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ emoji ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸
        severity_emoji = decoded['severity_emoji'] or 'ğŸ“‹'
        lines.append(f"{severity_emoji} NOTAM {decoded['id']} ({decoded['type_text']})")
        lines.append("")
        
        # ĞÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼
        if decoded['location_name']:
            lines.append(f"ğŸ“ ĞÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼: {decoded['location']} - {decoded['location_name']}")
        else:
            lines.append(f"ğŸ“ Ğ›Ğ¾ĞºĞ°Ñ†Ğ¸Ñ: {decoded['location']}")
        
        # Ğ¢Ğ¸Ğ¿ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ
        if decoded['q_decoded']:
            lines.append(f"âš ï¸  Ğ¢Ğ¸Ğ¿: {decoded['q_decoded']}")
        
        # ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ
        if decoded['is_permanent']:
            lines.append(f"â° Ğ”ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚: ĞŸĞĞ¡Ğ¢ĞĞ¯ĞĞĞ")
        else:
            valid_from = self._format_datetime(decoded['valid_from'])
            valid_to = self._format_datetime(decoded['valid_to'])
            lines.append(f"â° Ğ”ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚: Ñ {valid_from} Ğ´Ğ¾ {valid_to}")
        
        # Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
        if decoded['schedule']:
            lines.append(f"ğŸ“… Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: {decoded['schedule']}")
        
        # Ğ’Ñ‹ÑĞ¾Ñ‚Ñ‹
        if decoded['lower_limit'] or decoded['upper_limit']:
            lower = decoded['lower_limit'] or 'N/A'
            upper = decoded['upper_limit'] or 'N/A'
            lines.append(f"ğŸ“ Ğ’Ñ‹ÑĞ¾Ñ‚Ñ‹: {lower} - {upper}")
        
        lines.append("")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("ğŸ“ ĞĞŸĞ˜Ğ¡ĞĞĞ˜Ğ•:")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        
        # ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
        if decoded['description_decoded']:
            lines.append(decoded['description_decoded'])
        elif decoded['description_raw']:
            lines.append(decoded['description_raw'])
        
        # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸
        lines.append("")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        severity_text = {
            'critical': 'ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ - Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ',
            'warning': 'ğŸŸ¡ ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ• - Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ',
            'info': 'ğŸ”µ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯',
        }
        lines.append(severity_text.get(decoded['severity'], ''))
        
        return '\n'.join(lines)
    
    def _format_datetime(self, dt_str: Optional[str]) -> str:
        """
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ñ‹/Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
        
        Args:
            dt_str: ISO Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ°
            
        Returns:
            Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
        """
        if not dt_str:
            return 'N/A'
        
        if dt_str == 'EST':
            return 'ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ÑÑ'
        
        try:
            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ ISO Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
            if 'T' in dt_str:
                dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
                return dt.strftime('%d.%m.%Y %H:%M UTC')
        except Exception:
            pass
        
        return dt_str


# ========================================
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ Ğ ĞĞ‘ĞĞ¢Ğ« Ğ¡ Ğ¤ĞĞ™Ğ›ĞĞœĞ˜
# ========================================

def decode_notam_file(json_file: str, icao: Optional[str] = None) -> List[Dict]:
    """
    Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ NOTAM Ğ¸Ğ· JSON Ñ„Ğ°Ğ¹Ğ»Ğ°
    
    Args:
        json_file: ĞŸÑƒÑ‚ÑŒ Ğº JSON Ñ„Ğ°Ğ¹Ğ»Ñƒ
        icao: Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ ICAO (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
        
    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… NOTAM
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    decoder = NOTAMDecoder()
    decoded_list = []
    
    notams = data.get('notams', [])
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ICAO
    if icao:
        notams = [n for n in notams if n.get('location') == icao.upper()]
    
    for notam in notams:
        decoded = decoder.decode(notam)
        decoded_list.append(decoded)
    
    return decoded_list


# ========================================
# CLI
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ NOTAM Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:

  # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· JSON Ñ„Ğ°Ğ¹Ğ»Ğ°
  python3 notam_decoder.py --file notam_data.json
  
  # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑ€Ğ¾Ğ´Ñ€Ğ¾Ğ¼Ğ°
  python3 notam_decoder.py --file notam_data.json --icao UAAA
  
  # Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ
  python3 notam_decoder.py --file notam_data.json --json
  
  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°Ğ¹Ğ»
  python3 notam_decoder.py --file notam_data.json --output decoded.txt
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        type=str,
        required=True,
        help='JSON Ñ„Ğ°Ğ¹Ğ» Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ NOTAM'
    )
    
    parser.add_argument(
        '--icao',
        type=str,
        help='Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ ICAO ĞºĞ¾Ğ´Ñƒ'
    )
    
    parser.add_argument(
        '--json',
        action='store_true',
        help='Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² Ñ„Ğ°Ğ¹Ğ»'
    )
    
    args = parser.parse_args()
    
    # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    try:
        decoded_list = decode_notam_file(args.file, args.icao)
        
        if not decoded_list:
            print("âŒ NOTAM Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
            return
        
        if args.json:
            # JSON Ğ²Ñ‹Ğ²Ğ¾Ğ´
            output = json.dumps(decoded_list, ensure_ascii=False, indent=2)
            
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    f.write(output)
                print(f"âœ… Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ²: {args.output}")
            else:
                print(output)
        else:
            # Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´
            print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
            print("â•‘  ğŸ“‹ Ğ”Ğ•ĞšĞĞ”Ğ˜Ğ ĞĞ’ĞĞĞĞ«Ğ• NOTAM                                   â•‘")
            print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            print()
            
            for i, decoded in enumerate(decoded_list, 1):
                print(f"\n{'â•' * 60}")
                print(f"NOTAM {i} Ğ¸Ğ· {len(decoded_list)}")
                print('â•' * 60)
                print()
                print(decoded['human_readable'])
                print()
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°Ğ¹Ğ»
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    for i, decoded in enumerate(decoded_list, 1):
                        f.write(f"\n{'â•' * 60}\n")
                        f.write(f"NOTAM {i} Ğ¸Ğ· {len(decoded_list)}\n")
                        f.write('â•' * 60 + '\n\n')
                        f.write(decoded['human_readable'] + '\n\n')
                
                print(f"âœ… Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ²: {args.output}")
    
    except FileNotFoundError:
        print(f"âŒ Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {args.file}")
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
